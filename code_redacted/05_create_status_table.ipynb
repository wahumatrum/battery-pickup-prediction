{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "%load_ext sql\n",
    "import prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions we can not yet put in a module (have to copy for every notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_milli_time():\n",
    "    return round(time.time() * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection_point_dataframes(data, unique_names=True):\n",
    "    \"\"\"_summary_\n",
    "    Creates a lot of dataframes split by collection point number (übergabestellennummer)\n",
    "     in the global namespace. Using the returned list of dataframe names you can call\n",
    "        df = globals()[collection_point_df_name_list[x]] to get any of the dataframes.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : _type_\n",
    "        _description_\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _type_\n",
    "        _description_\n",
    "    list of dataframe names that are created globally.\n",
    "    \"\"\"  \n",
    "    collection_points = list(data.übergabestellennummer.unique())\n",
    "    collection_points.sort()\n",
    "    # create a list for the variable names\n",
    "    collection_point_df_name_list = list()\n",
    "\n",
    "    for cp in collection_points:\n",
    "        if(unique_names):\n",
    "            dataframe_name = str(current_milli_time()) + \"_orders_collection_point_\" + cp.astype(str)\n",
    "        else:\n",
    "            dataframe_name = \"orders_collection_point_\" + cp.astype(str)\n",
    "        collection_point_df_name_list.append(dataframe_name)\n",
    "        # The df is assigned to \"orders_collection_point_<some-collection-point-id>\" variable\n",
    "        Dynamic_Variable_Name = dataframe_name\n",
    "        globals()[Dynamic_Variable_Name] = data.query(\"übergabestellennummer == @cp\")\n",
    "    return collection_point_df_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_table(cp_list, table, debug=False, debug_sample_size=1):\n",
    "    \n",
    "    number_collection_points = debug_sample_size\n",
    "    count = 0\n",
    "    error_list = list()\n",
    "    for df_name in cp_list:\n",
    "        if debug:\n",
    "            if count >= number_collection_points:\n",
    "                break\n",
    "        df = globals()[df_name]\n",
    "        if(debug):\n",
    "            print(f\"df_name: {df_name} has {df.shape[0]} rows\")\n",
    "        df = prediction.drop_single_pick_ups_and_single_initial_deliveries(df)\n",
    "        if df.shape[0] > 0:\n",
    "            if(debug):\n",
    "                print(f\"df_name: {df_name} has {df.shape[0]} rows\")\n",
    "            stats_dict = prediction.predict_capacity_of_collection_point_full_date(df, debug=debug)\n",
    "            if(debug):\n",
    "                print(stats_dict)\n",
    "            table = table.append(stats_dict, ignore_index = True)\n",
    "            count += 1  \n",
    "    \n",
    "    return table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### notebook code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_pickle(\"../data/battery_cleaned_with_geo.pkl\")\n",
    "orders = prediction.filter_for_report(orders)\n",
    "orders_comp, orders_open = prediction.filter_dataframe_for_prediction(orders)\n",
    "orders_comp = prediction.remove_orders_with_unknown_weights(orders_comp)\n",
    "orders_comp = orders_comp.sort_values(by=['konzernnummer','übergabestellennummer', 'abholdatum'], ascending=True)\n",
    "cp_list = create_collection_point_dataframes(orders_comp, unique_names=False)\n",
    "\n",
    "# redacted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug single collection point\n",
    "cp = 109991\n",
    "df = globals()[\"orders_collection_point_\" + str(cp)]\n",
    "\n",
    "prediction.predict_capacity_of_collection_point_full_date(df, debug=True)\n",
    "df[[\"übergabestellennummer\", \"konzernnummer\", \"vertragsnummer\", \"auftragsnummer\", \"auftragsstatus\", \"auftragstyp\", \"abholdatum\", \"nettogewicht_in_kg\", \"angemeldete_containeranzahl\", \"angeforderter_behältertyp\", \"gelieferter_behältertyp\", \"gelieferte_behälteranzahl\"]]\n",
    "\n",
    "# redacted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame()\n",
    "db_data = create_db_table(cp_list, table, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_data\n",
    "# redacted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_data.info()\n",
    "# redacted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for inf values\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "newdf = db_data.select_dtypes(include=numerics)\n",
    "newdf\n",
    "# redacted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "printing column name where infinity is present\n",
      "Series([], dtype: object)\n",
      "\n",
      "printing row index with infinity \n",
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# check for inf values\n",
    "\n",
    "# printing column name where infinity is present\n",
    "print()\n",
    "print(\"printing column name where infinity is present\")\n",
    "col_name = newdf.columns.to_series()[np.isinf(newdf).any()]\n",
    "print(col_name)\n",
    "  \n",
    "# printing row index with infinity\n",
    "print()\n",
    "print(\"printing row index with infinity \")\n",
    "  \n",
    "r = newdf.index[np.isinf(newdf).any(1)]\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show rows with inf values\n",
    "db_data_checked = db_data[[\"übergabestellennummer\", \"tägl_smenge_kg\", \"erreicht_kg\", \"erreicht_prozent\"]].iloc[r]\n",
    "db_data_checked.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store results into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "load_dotenv()\n",
    "DATABASE = os.getenv('DATABASE')\n",
    "USER_DB = os.getenv('USER_DB')\n",
    "PASSWORD = os.getenv('PASSWORD')\n",
    "HOST = os.getenv('HOST')\n",
    "PORT = os.getenv('PORT')\n",
    "\n",
    "# connect to existing db\n",
    "con_string = \"mysql+pymysql://\" + USER_DB + \":\" + PASSWORD + \"@\" + HOST + \"/\" + DATABASE + \"?charset=utf8mb4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>battery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>information_schema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mysql</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>performance_schema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sys</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Database\n",
       "0             battery\n",
       "1  information_schema\n",
       "2               mysql\n",
       "3  performance_schema\n",
       "4                 sys"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = create_engine(con_string)\n",
    "# test connection\n",
    "pd.read_sql(\"show databases\", db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_NAME = \"STATUS_COLLECTION_POINT_2\"\n",
    "OVERWRITE_TABLE = False\n",
    "if OVERWRITE_TABLE:\n",
    "    pd.read_sql(\"drop table \" + TABLE_NAME, db)\n",
    "    db_data.to_sql(TABLE_NAME, db, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_data.to_excel(\"../data/status_sammelstellen.xlsx\", sheet_name='status')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9b1f7ee9b8716eae474e4e403dd4dff0f662765920b174d1bb68198c0d2acd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
